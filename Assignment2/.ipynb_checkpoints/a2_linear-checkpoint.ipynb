{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS549 Machine Learning \n",
    "# Assignment 2: Linear Regression Model\n",
    "\n",
    "**Total: 5 points**\n",
    "\n",
    "The goal of this homework assignment is to practice the Python implementation the **gradient descent** method for training linear regression models. See detailed instructions below. \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "The task is to build a linear regression model that predicts the GPAs of university students from two features, Math SAT and Verb SAT. \n",
    "- Task 1) Prepare the feature matrix $X$ and label vector $y$.\n",
    "- Task 2) Train the model using Gradient Descent method.\n",
    "\n",
    "## Datasets\n",
    "The file *sat_gpa.csv* contains all training and testing data. It has 105 rows and 3 columns. Each row is the record of a student. The three columns are <u>Math SAT score</u>, <u>Verb SAT score</u>, and <u>University GPA</u>. The first two columns are the features, and the third is the label. All data points are used as the training set.\n",
    "\n",
    "---\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv # Used for computing the inverse of matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Use `pip install matplotlib` in command line if matplotlib is not installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of original data: (105, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load data \n",
    "data = np.loadtxt(open('sat_gpa.csv'), delimiter=',')\n",
    "print('shape of original data:', data.shape) # Check if data is 105 by 3\n",
    "\n",
    "# Normalize data\n",
    "data_norm = data / data.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1\n",
    "**1 point**\n",
    "\n",
    "Prepare the feature matrix $X$ and label vector $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create matrix X and y\n",
    "# X has three columns: \n",
    "#   - The first column contain all 1s, which is for the intercept\n",
    "#   - The second and third columns contain features, i.e., the 1st and 2nd columns of data_norm\n",
    "# y has one column, which is the 3rd column of data_norm\n",
    "\n",
    "X = np.ones_like(data_norm)\n",
    "\n",
    "#### START YOUR CODE ####\n",
    "X[:, 1:3] = data_norm[:, 0:2]\n",
    "y = data_norm[:,2]\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Check if the 2nd and 3rd columns of X have equal values as the 1st and 2nd columns of data_norm\n",
    "print(np.array_equal(X[:,1:3], data_norm[:,0:2]))\n",
    "# Check if y equals the last column of data_norm\n",
    "print(np.array_equal(y, data_norm[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected ouput\n",
    "True<br>\n",
    "True\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "**4 points**\n",
    "\n",
    "Implement the Gradient Descent method for linear regression.\n",
    "\n",
    "The cost function: $J(\\theta_0, \\theta_1, \\theta_2) = \\frac{1}{2m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2m}\\sum_i (\\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Gradients w.r.t. parameters: $\\frac{\\partial J}{\\partial \\theta} = \\begin{cases}\\frac{\\partial J}{\\partial \\theta_0}\\\\ \\frac{\\partial J}{\\partial \\theta_1}\\\\ \\frac{\\partial J}{\\partial \\theta_2}\\\\ \\end{cases} = \\begin{cases}\\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})\\\\ \\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})x_1^{(i)}\\\\ \\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})x_2^{(i)}\\\\\\end{cases}$\n",
    "\n",
    "The formula to update parameters at each iteration: $\\theta := \\theta - \\alpha * \\frac{\\partial J}{\\partial \\theta}$\n",
    "\n",
    "Note that $X$, $y$, and $\\theta$ are all vectors (numpy arrays), and thus the operations above should be implemented in a vectorized fashion. Use `numpy.sum()`, `numpy.dot()` and other vectorized functions, and avoid writing `for` loops in Python.\n",
    "\n",
    "Use the learned $\\theta$ to make predictions: $\\hat{y} = X\\theta$\n",
    "\n",
    "Compute the residual sum of squares of the model: $RSS = \\sum_i (\\hat{y}^{(i)} - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradientDescent function\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Params\n",
    "        X - Shape: (m,3); m is the number of data examples\n",
    "        y - Shape: (m,)\n",
    "        theta - Shape: (3,)\n",
    "        num_iters - Maximum number of iterations\n",
    "    Return\n",
    "        A tuple: (theta, RSS, cost_array)\n",
    "        theta - the learned model parameters\n",
    "        RSS - residual sum of squares\n",
    "        cost_array - stores the cost value of each iteration. Its shape is (num_iters,)\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_array =[]\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "        #### START YOUR CODE ####\n",
    "        # Make predictions\n",
    "        # Shape of y_hat: m by 1\n",
    "        # y_hat = np.random.randint(1, 4, size=(m,1), dtype='float')\n",
    "        y_hat = np.dot(X, theta)\n",
    "        \n",
    "        # Compute the difference between prediction (y_hat) and ground truth label (y)\n",
    "        diff = np.subtract(y_hat, y)\n",
    "       \n",
    "        # Compute the cost\n",
    "        # Hint: Use the diff computed above\n",
    "        cost = (1/(2*m)) * np.sum(diff ** 2)\n",
    "        cost_array.append(cost)\n",
    "\n",
    "        # Compute gradients\n",
    "        # Hint: Use the diff computed above\n",
    "        # Hint: Shape of gradients is the same as theta\n",
    "        gradients = np.dot(X.transpose(), diff) / m\n",
    "\n",
    "        # Update theta\n",
    "        theta = theta - (alpha * gradients)\n",
    "        \n",
    "        #### END YOUR CODE ####\n",
    "        \n",
    "    # Compute residuals\n",
    "    # Hint: Should use the same code as Task 1\n",
    "    #### START YOUR CODE ####\n",
    "    y_hat = np.dot(X, theta)\n",
    "    RSS = np.sum(diff ** 2)\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "    return theta, RSS, cost_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta obtained from gradient descent: [0.29911574 0.32224209 0.31267172]\n",
      "Residual sum of squares (RSS):  0.8642105973800466\n"
     ]
    }
   ],
   "source": [
    "# This cell is to evaluate the gradientDescent function implemented above\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Define learning rate and maximum iteration number\n",
    "ALPHA = 0.05\n",
    "MAX_ITER = 500\n",
    "\n",
    "# Initialize theta to [0,0,0]\n",
    "theta = np.zeros(3)\n",
    "theta_GD, RSS, cost_array = gradientDescent(X, y, theta, ALPHA, MAX_ITER)\n",
    "\n",
    "print('Theta obtained from gradient descent:', theta_GD)\n",
    "print('Residual sum of squares (RSS): ', RSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "Theta obtained from gradient descent: | [0.29911574 0.32224209 0.31267172]\n",
    "Residual sum of squares (RSS): | 0.8641600584370602"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
